# based on this github : https://github.com/nicknochnack/Langchain-Crash-Course
# Bring in deps
import os 

import streamlit as st 
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, SequentialChain 
from langchain.memory import ConversationBufferMemory
from langchain.utilities import SerpAPIWrapper
from langchain.agents import Tool
from langchain.tools.file_management.write import WriteFileTool
from langchain.tools.file_management.read import ReadFileTool
    
os.environ['OPENAI_API_KEY'] = st.secrets["auth"]

# App framework
st.set_page_config(
   page_title="quizForge",
   page_icon="üßä",
)

st.title('üîóüí¨ Quiz FORGE')

st.write('üîóüí¨ G√âN√âRATEUR DE QUIZ')
st.write("QuizForge est un outil d'IA con√ßu pour aider les enseignants √† cr√©er des questionnaires pertinents pour leurs cours.")
st.info("QuizForge demande les informations ci-dessous afin de g√©n√©rer le contenu le plus pr√©cis possible pour vous. Toutes les informations sont facultatives et contribuent √† am√©liorer la pr√©cision de la sortie g√©n√©r√©e.")

st.title('D√©crivez votre cours')
st.info("Dans cette zone de saisie, vous pouvez fournir des informations sur votre cours et votre mati√®re pour aider QuizForge √† g√©n√©rer un questionnaire pr√©cis pour vous.")
prompt_title = st.text_input('Th√©matique de cours') 



# Prompt templates
title_template = PromptTemplate(
    input_variables = ['topic'], 
    template = "En tant qu'expert en enseignement et en p√©dagogie, vous √™tes charg√©(e) de concevoir cinq questions √† choix multiple pour √©valuer la compr√©hension d'un sujet sp√©cifique. Pour chaque question, veuillez fournir uniquement la question elle-m√™me, sans les r√©ponses ni les propositions de r√©ponses. Voici la th√©matique : \n\n1. {topic}"

)

# Prompt templates
question_template = PromptTemplate(
    input_variables = ['list_question', 'number'], 
  template = "√Ä partir de la liste de questions suivante : {list_question}, reprenez la question {number}, reformulez-la et proposez quatre r√©ponses possibles. Indiquez laquelle parmi ces r√©ponses est la correcte. Veuillez √©num√©rer les r√©ponses une par une, en sautant une ligne entre chacune, et fournir une explication d√©taill√©e pour la r√©ponse correcte."

)


# Memory 
title_memory = ConversationBufferMemory(input_key='topic', memory_key='chat_history')
listq_memory = ConversationBufferMemory(input_key='list_question', memory_key='list_question_history')

# Llms
llm = OpenAI(temperature=0.9, model_name="gpt-3.5-turbo-16k")
 
title_chain = LLMChain(llm=llm, prompt=title_template, verbose=True, output_key='list_question', memory=title_memory)
question_chain = LLMChain(llm=llm, prompt=question_template, verbose=True, output_key='question', memory=listq_memory)

# Show stuff to the screen if there's a prompt
if st.button('G√©n√©rer les questions'):

    list_question = title_chain.run(prompt_title)
    st.info('Liste des questions : \n' +  list_question)
    number = 1
    question = question_chain.run(list_question=list_question, number=number )
    st.info('Question 1 : \n' +  question)

    number = 2
    question = question_chain.run(list_question=list_question, number=number )
    st.info('Question 2 : \n' +  question)

    number = 3
    question = question_chain.run(list_question=list_question, number=number )
    st.info('Question 3 : \n' +  question)

    number = 4
    question = question_chain.run(list_question=list_question, number=number )
    st.info('Question 4 : \n' +  question)

    number = 5
    question = question_chain.run(list_question=list_question, number=number )
    st.info('Question 5 : \n' +  question)
